<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">	  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">	  
  <link rel="icon" type="image/png" href="resource/umd_logo.png">
  <script type="text/javascript" src="resource/hidebib.js"></script>
  <title>Chethan Parameshwara</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Chethan Parameshwara</name>
        </p>
        <p> </p>
	<p>I work as a researcher at <a href="https://www.amazon.science/">Amazon</a>, focusing on Multimodal Generative AI, with a specific emphasis on Video Diffusion Models and 3D Computer Vision.</p>
		
	<p>
	Before joining Amazon, I graduated with a Ph.D. in Neuroscience and Artificial Intelligence (<a href="https://nacs.umd.edu/">NACS</a>) from the University of Maryland, College Park (<a href="https://umd.edu/">UMD</a>), 
	where I developed neuroscience-inspired computer vision algorithms for autonomous vehicles.	
	</p>
		
		
        <p align=center>
          <a href="mailto:cmparam9@terpmail.umd.edu">Email</a> &nbsp/&nbsp
          <a href="https://github.com/chethanparameshwara">GitHub</a> &nbsp/&nbsp
          <!--a href="">Thesis</a-->
          <a href="https://scholar.google.com/citations?user=tStSA4AAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp 
          <a href="http://www.linkedin.com/in/cmparam/"> LinkedIn </a> &nbsp/&nbsp
	  <a href="https://twitter.com/cmparam"> Twitter </a>
        </p>
        </td>
              <td width="66%">
                <img class="img-circle" height="160px" src="resource/chethan_profilepic.jpg">
              </td>
      </tr>
      </table>

        <table width="100%" align="right" border="0" cellspacing="0" cellpadding="20" style="border-collapse: collapse; margin-top:10px">
          <tr style="border-bottom: 2px solid rgb(192, 192, 192)">
          <td width="100%" valign="top" align="middle">
            <!--a href="#research"><big><b>Research Interests</b></big></a-->
            <a href="#publication"><big><b>Publications</b></big></a> &emsp;<span>&#183;</span>&emsp;
	    <a href="#experiences"><big><b>Experiences</b></big></a> &emsp;<span>&#183;</span>&emsp;
            <a href="#teaching"><big><b>Teaching</b></big></a> &emsp;<span>&#183;</span>&emsp;
            <a href="#volunteering"><big><b>Volunteering</b></big></a>
            </td>
          </td>
          </td>
          </tr>
        </table>

	  
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications</heading>
	  <div id="publication" style="width: 100%; margin-top: 10px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; display: inline-block">
          </div>
          <p> My selected publications are listed here. The complete list of publications can be seen from my <a href="https://scholar.google.com/citations?user=tStSA4AAAAAJ&hl=en">Google Scholar</a> page.
              </p>
      </td>
      </tr>
      </table>

	  
	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	
    <tr onmouseout="nerf_stop()" onmouseover="nerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nerf_image'>
            <img src='resource/nerf_after.png' width="160" height="160" style="border-style: none">
          </div>
          <img src='resource/nerf_before.png' width="160" height="160" style="border-style: none">
        </div>
        <script type="text/javascript">
          function nerf_start() {
            document.getElementById('nerf_image').style.opacity = "1";
          }

          function nerf_stop() {
            document.getElementById('nerf_image').style.opacity = "0";
          }
          nerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2105.06562">
          <papertitle>SpikeMS: Deep Spiking Neural Network for Motion Segmentation</papertitle>
        </a>
        <br>
        <strong>Chethan M. Parameshwara*</strong>,
  <a href="https://www.linkedin.com/in/simin-li-88088b/">Simin Li*</a>,
  <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,
        <a href="https://scholar.google.com/citations?user=ugyWSWwAAAAJ&hl=en">Nitin J. Sanket</a>,
  <a href="https://www.linkedin.com/in/matthew-evanusa-8772109b/">Matthew S. Evanusa</a>,
  <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a> (* equal contribution)		
        <br>
  <em><a href=" https://www.iros2021.org/">IROS, 2021</a></em> 
        <br>
        <a href="https://prg.cs.umd.edu/SpikeMS">project page</a> /
        <a href="https://github.com/prgumd/SpikeMS">code</a> /
        <a href="https://arxiv.org/abs/2105.06562">arXiv</a> /
        <a href="https://www.youtube.com/watch?v=q3w3ns9hWl8">video</a>
  
        <p></p>
        <p style="color:rgb(110, 110, 110)"> We propose a bio-inspired neural network for the motion detection problem using an asynchronous event camera.</p>
      </td>
    </tr>  

		<tr onmouseout="diffposenet_stop()" onmouseover="diffposenet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='diffposenet_image'><img src='resource/diffposenet.gif'></div>
                <img src='resource/diffposenet.png'>
              <script type="text/javascript">
                function diffposenet_start() {
                  document.getElementById('diffposenet_image').style.opacity = "1";
                }

                function diffposenet_stop() {
                  document.getElementById('diffposenet_image').style.opacity = "0";
                }
                diffposenet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ter.ps/diffposenet">
                <papertitle>DiffPoseNet: Direct Differentiable Camera Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Chethan M. Parameshwara</strong>,
	      <a href="https://scholar.google.com/citations?user=mkap0DoAAAAJ&hl=en">Gokul Hari</a>,
	      <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,
              <a href="https://scholar.google.com/citations?user=ugyWSWwAAAAJ&hl=en">Nitin J. Sanket</a>,
	      <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a>		
              <br>
        <em><a href=" https://cvpr2022.thecvf.com/">CVPR, 2022</a></em> 
              <br>
              <a href="https://prg.cs.umd.edu/DiffPoseNet">project page</a> /
              <a href="https://arxiv.org/abs/2203.11174">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=EuK-7WaM1mQ">video</a>
		    
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We propose a novel differentiable programming technique for camera pose estimation in autonomous driving and flying scenarios.</p>
            </td>
          </tr>  
	   <!--tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resource/momswithevents.png" alt="b3do" width="160" style="border-style: none">
            </td-->
	<tr onmouseout="spikems_stop()" onmouseover="spikems_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spikems_image'>
                  <img src='resource/spikems_output.png' width="160" height="160" style="border-style: none">
                </div>
                <img src='resource/spikems_input.png' width="160" height="160" style="border-style: none">
              </div>
              <script type="text/javascript">
                function spikems_start() {
                  document.getElementById('spikems_image').style.opacity = "1";
                }

                function spikems_stop() {
                  document.getElementById('spikems_image').style.opacity = "0";
                }
                spikems_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2105.06562">
                <papertitle>SpikeMS: Deep Spiking Neural Network for Motion Segmentation</papertitle>
              </a>
              <br>
              <strong>Chethan M. Parameshwara*</strong>,
	      <a href="https://www.linkedin.com/in/simin-li-88088b/">Simin Li*</a>,
	      <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,
              <a href="https://scholar.google.com/citations?user=ugyWSWwAAAAJ&hl=en">Nitin J. Sanket</a>,
	      <a href="https://www.linkedin.com/in/matthew-evanusa-8772109b/">Matthew S. Evanusa</a>,
	      <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a> (* equal contribution)		
              <br>
        <em><a href=" https://www.iros2021.org/">IROS, 2021</a></em> 
              <br>
              <a href="https://prg.cs.umd.edu/SpikeMS">project page</a> /
              <a href="https://github.com/prgumd/SpikeMS">code</a> /
              <a href="https://arxiv.org/abs/2105.06562">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=q3w3ns9hWl8">video</a>
		    
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We propose a bio-inspired neural network for the motion detection problem using an asynchronous event camera.</p>
            </td>
          </tr>  
		 
	<tr onmouseout="nudgeseg_stop()" onmouseover="nudgeseg_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nudgeseg_image'><img src='resource/nudgeseg.gif'></div>
                <img src='resource/nudgeseg.png'>
              </div>
              <script type="text/javascript">
                function nudgeseg_start() {
                  document.getElementById('nudgeseg_image').style.opacity = "1";
                }

                function nudgeseg_stop() {
                  document.getElementById('nudgeseg_image').style.opacity = "0";
                }
                nudgeseg_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.youtube.com/watch?v=wQSS5fjKGbQ">
                <papertitle>NudgeSeg: Zero-Shot Object Segmentation by Repeated Physical Interaction</papertitle>
              </a>
              <br>
	      <a href="https://scholar.google.com/citations?user=ASaUMpsAAAAJ&hl=en">Chahat Deep Singh*</a>,
              <a href="https://scholar.google.com/citations?user=ugyWSWwAAAAJ&hl=en">Nitin J. Sanket*</a>,
	      <strong>Chethan M. Parameshwara</strong>,
              <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,	   
	      <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a> (* equal contribution)
              <br>
        	<em><a href="https://www.iros2021.org/">IROS, 2021</a></em>  
              <br>
              <a href="http://prg.cs.umd.edu/NudgeSeg">project page</a> /
              <a href="https://arxiv.org/pdf/2109.13859.pdf">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=wQSS5fjKGbQ">video</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We present a novel approach to segment never-seen objects by physically interacting with them and observing them from different views.</p>
            </td>
          </tr>  
		 
		 
	  <tr onmouseout="mms_stop()" onmouseover="mms_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mms_image'><img src='resource/0-mms.gif'></div>
                <img src='resource/0-mms.png'>
              </div>
              <script type="text/javascript">
                function mms_start() {
                  document.getElementById('mms_image').style.opacity = "1";
                }

                function mms_stop() {
                  document.getElementById('mms_image').style.opacity = "0";
                }
                mms_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.06158">
                <papertitle>0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera</papertitle>
              </a>
              <br>
              <strong>Chethan M. Parameshwara</strong>,
              <a href="https://scholar.google.com/citations?user=ugyWSWwAAAAJ&hl=en">Nitin J. Sanket</a>,
              <a href="https://scholar.google.com/citations?user=ASaUMpsAAAAJ&hl=en">Chahat Deep Singh</a>,
              <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,
	      <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a>
              <br>
        	<em><a href="https://www.ieee-icra.org/">ICRA, 2021</a></em>  
              <br>
              <a href="http://prg.cs.umd.edu/0-MMS">project page</a> /
              <a href="https://github.com/prgumd/0-MMS">code</a> /
              <a href="https://arxiv.org/abs/2006.06158">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=KdpZkxjp02E">video</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We propose a hybrid solution to multi-object motion segmentation using a combination of model-based and deep learning approaches with minimal prior knowledge.</p>
            </td>
          </tr>  
		 
		 
			 
	   <tr onmouseout="evpropnet_stop()" onmouseover="evpropnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='evpropnet_image'><img src='resource/evpropnet.gif'></div>
                <img src='resource/evpropnet.png'>
              </div>
              <script type="text/javascript">
                function evpropnet_start() {
                  document.getElementById('evpropnet_image').style.opacity = "1";
                }

                function evpropnet_stop() {
                  document.getElementById('evpropnet_image').style.opacity = "0";
                }
                evpropnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2106.15045">
                <papertitle>EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=ugyWSWwAAAAJ&hl=en">Nitin J. Sanket</a>,
              <a href="https://scholar.google.com/citations?user=ASaUMpsAAAAJ&hl=en">Chahat Deep Singh</a>,
	      <strong>Chethan M. Parameshwara</strong>,
              <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,
	      <a href="https://scholar.google.nl/citations?user=gTF1wiwAAAAJ&hl=en"> Guido de Croon</a>,	   
	      <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a>
              <br>
        	<em><a href="https://roboticsconference.org/">RSS, 2021</a></em>  
              <br>
              <a href="https://prg.cs.umd.edu/EVPropNet">project page</a> /
              <a href="https://github.com/prgumd/EVPropNet">code</a> /
              <a href="https://arxiv.org/abs/2106.15045">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=WpRB_3dyXTs">video</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We present a deep learning-based solution for detecting propellers (to detect drones) for mid-air landing and following.</p>
            </td>
          </tr>  
		   
		   
	   <tr onmouseout="evdodge_stop()" onmouseover="evdodge_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='evdodge_image'><img src='resource/evdodge.gif'></div>
                <img src='resource/evdodge.png'>
              </div>
              <script type="text/javascript">
                function evdodge_start() {
                  document.getElementById('evdodge_image').style.opacity = "1";
                }

                function evdodge_stop() {
                  document.getElementById('evdodge_image').style.opacity = "0";
                }
                evdodge_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://prg.cs.umd.edu/EVDodgeNet">
                <papertitle>EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras</papertitle>
              </a>
              <br>
              <strong>Chethan M. Parameshwara*</strong>,
              <a href="https://scholar.google.com/citations?user=ugyWSWwAAAAJ&hl=en">Nitin J. Sanket*</a>,
              <a href="https://scholar.google.com/citations?user=ASaUMpsAAAAJ&hl=en">Chahat Deep Singh</a>,
              <a href="">Ashwin V. Kuruttukulam</a>,
              <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,
	      <a href="https://scholar.google.com/citations?user=SC9wV2kAAAAJ&hl=en">Davide Scaramuzza</a>,
	      <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a>
	      (* equal contribution)
              <br>
	      <em><a href="https://www.ieee-ras.org/students/events/event/1144-icra-2020-ieee-international-conference-on-robotics-and-automation-icra">ICRA, 2020</a></em>
  
              <br>
              <a href="http://prg.cs.umd.edu/EVDodgeNet">project page</a>
        /
              <a href="https://github.com/prgumd/EVDodgeNet">code</a>
        /
              <a href="https://arxiv.org/abs/1906.02919">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=NSwK1ZEsTOo">video</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We present the first deep learning based solution for dodging multiple dynamic obstacles on a quadrotor with a single event camera and onboard computation.</p>
            </td>
          </tr>  
	
	    
	 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		 
	   <!--tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resource/betterflow.PNG" alt="b3do" width="160" style="border-style: none">
            </td-->
	 <tr onmouseout="betterflow_stop()" onmouseover="betterflow_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='betterflow_image'>
                  <img src='resource/betterflow_after.png' width="160" height="160" style="border-style: none">
                </div>
                <img src='resource/betterflow_before.png' width="160" height="160" style="border-style: none">
              </div>
              <script type="text/javascript">
                function betterflow_start() {
                  document.getElementById('betterflow_image').style.opacity = "1";
                }

                function betterflow_stop() {
                  document.getElementById('betterflow_image').style.opacity = "0";
                }
                betterflow_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://prg.cs.umd.edu/BetterFlow">
                <papertitle>Event-based Moving Object Detection and Tracking</papertitle>
              </a>
              <br>
	      <a href="https://scholar.google.com/citations?user=ztTOsgkAAAAJ&hl=en">Anton Mitrokhin</a>,    
	      <a href="https://scholar.google.com/citations?user=0gEOJSEAAAAJ">Cornelia Fermüller</a>,
              <strong>Chethan M. Parameshwara</strong>,
	      <a href="https://scholar.google.com/citations?user=7QmEsOwAAAAJ&hl=en">Yiannis Aloimonos</a>
              <br>
        <em><a href=" https://www.iros2018.org/">IROS, 2018</a></em> 
              <br>
              <a href="http://prg.cs.umd.edu/BetterFlow">project page</a>
        /
              <a href="https://github.com/better-flow/better-flow">code</a>
        /
              <a href="https://arxiv.org/pdf/1803.04523.pdf">arXiv</a>
        /
              <a href="https://www.youtube.com/watch?v=UCAJi0ZFaZ8">video</a>
              <p></p>
              <p style="color:rgb(110, 110, 110)"> We present a novel motion compensation approach for moving object tracking with an asynchronous event camera.</p>
            </td>
          </tr> 
         </tbody></table>
	<table id="experiences" width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="margin-top:10px">
            <tr>
              <td width="100%" valign="middle">
                <heading>Experiences</heading>
              </td>
            </tr>
        </table>
		 
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
              <td width="10%" align="center"><img src="resource/logos/amazon-logo.jpg" alt="PontTuset" width="100" style="border-style: none">
                <td width="100%" valign="top">
                    <p><papertitle><a href="https://www.amazon.science/" target="_blank">Amazon</a></papertitle>  
                      <br>
                      Aug, 2022 -- Present
                      <br>
                      <strong>Applied Scientist II</strong>
                    </p>	
		    </p>
                    <p style="color:rgb(110, 110, 110)">
                     Research on Video Diffusion Models (text-to-video, image-to-video, video-to-video) and 3D Computer Vision (NeRF, Gaussian Splatting, Neural Rendering, and SLAM).
		 </p>
                </td>
	  
	  
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
              <td width="10%" align="center"><img src="resource/logos/umd.png.jpg" alt="PontTuset" width="100" style="border-style: none">
                <td width="100%" valign="top">
                    <p><papertitle><a href="https://umd.edu/" target="_blank">University of Maryland, College Park (UMD)</a></papertitle>  
                      <br>
                      Aug, 2017 -- Aug, 2022
                      <br>
                      <strong>Graduate Research Assistant</strong> <em>with</em> <a href="http://www.cfar.umd.edu/~yiannis/" target="_blank">Prof. Yiannis Aloimonos</a> and <a href="http://www.cfar.umd.edu/~fer/" target="_blank">Dr. Cornelia Fermüller</a>
                    </p>	
		    </p>
                    <p style="color:rgb(110, 110, 110)">
                     Developed neuromorphic motion segmentation and tracking algorithms for high-speed and challenging lighting scenarios and also developed differentiable optimization layers (a combination of model-based and data-driven) for camera pose estimation to improve robustness and generalization across scenes.
                    </p>
                </td>
	  
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
              <td width="10%" align="center"><img src="resource/logos/SRI.png" alt="PontTuset" width="100" style="border-style: none">
                <td width="100%" valign="top">
                    <p><papertitle><a href="https://www.sri.com/" target="_blank">SRI International (formerly Stanford Research Institute)</a></papertitle>  
                      <br>
                      Summer, 2021
                      <br>
                      <strong>Research Intern</strong> <em>with</em> <a href="https://www.linkedin.com/in/david-zhang-99217920/" target="_blank">Dr. David Zhang</a> and <a href="https://www.sri.com/bios/michael-piacentino/" target="_blank">Michael Piacentino</a>
                    </p>	
		    </p>
                    <p style="color:rgb(110, 110, 110)">
                     Developed a neuro-inspired learning approach for few-shot image classification with a faster convergence rate (10x) and consumes low memory (20x) than existing image classification algorithms. 
                    </p>
                </td>
		  
	      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
              <td width="10%" align="center"><img src="resource/logos/neurala.png" alt="PontTuset" width="100" style="border-style: none">
                <td width="100%" valign="top">
                    <p><papertitle><a href="https://www.neurala.com/" target="_blank">Neurala</a></papertitle>  
                      <br>
                      Summer, 2019
                      <br>
                      <strong>Research Intern</strong> <em>with</em> <a href="https://www.linkedin.com/in/anatoli-gorchet-427a0720/" target="_blank">Dr. Anatoli Gorchet</a> and <a href="https://scholar.google.com/citations?user=lDAWr5YAAAAJ&hl=en" target="_blank">Dr. Matthew Luciw</a>
                    </p>
		  <p style="color:rgb(110, 110, 110)">
                      Developed a lifelong machine learning approach to improve few-shot learning capabilities for object detection tasks and deployed the proposed approach on Neurala's Brain Builder software.
			</p>
                </td>
	     
		    
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="border-collapse: collapse">
              <td width="10%" align="center"><img src="resource/logos/rta.png" alt="PontTuset" width="100" style="border-style: none">
                <td width="100%" valign="top">
                    <p><papertitle><a href="https://www.youtube.com/watch?v=VRO_4NtVsoU" target="_blank">Robot Training Academy</a></papertitle>  
                      <br>
                      Fall, 2016
                      <br>
                      <strong>Software Engineering Intern</strong>
                    </p>
		    <p style="color:rgb(110, 110, 110)">
                      Developed hand gesture tracking software for human-robot interaction in kitchen environments and performed testing of perception software modules on Rethink Baxter robot. </p>
                </td>

		  

<!--Teaching-->
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
	      <div id="teaching" style="width: 100%; margin-top: 10px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; display: inline-block"> </div>
            </td>
          </tr>
        </tbody></table>
	<table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="resource/intrinsic-pinhole-camera.png" alt="panoroma" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              Teaching Assistant, <a href="http://prg.cs.umd.edu/cmsc733">CMSC733 : Geometric Computer Vision</a>
              <br>
              <br>
	      Teaching Assistant, <a href="http://prg.cs.umd.edu/cmsc426">CMSC426 : Computer Vision</a>
              <br>
	      <br>
              Teaching Assistant, <a href="https://umd.instructure.com/courses/1257184">CMSC434 : Human Computer Interaction</a>
            </td>
          </tr>
        </tbody></table>
	   	    
<!--Service -->
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Volunteering</heading>
	      <div id="volunteering" style="width: 100%; margin-top: 10px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; display: inline-block"> </div>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="resource/umd_logo.png"  alt="gsglogo" width="160" height="160">
	    </td>
            <td width="75%" valign="middle">
		    Reviewer: <a href="https://nips.cc/">NeurIPS</a>, <a href="https://iclr.cc/">ICLR</a>, <a href="https://cvpr2022.thecvf.com/">CVPR</a>, <a href="https://eccv2022.ecva.net/">ECCV</a>, <a href="https://www.ieee-ras.org/publications/ra-l">RAL</a>, <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra/">ICRA</a>, <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros">IROS</a>, <a href="https://ieee-sensors.org/sensors-journal/">IEEE Sensor Journal</a>
              <br>
              <br>
              NACS Representative, <a href="https://www.gsgumd.org/">UMD Graduate Student Government (2020-2021)</a>
              <br>
              <br>
              Co-Chair, <a href="https://nacs.umd.edu/about-us/committees">NACS Grant Review Committee (2019-2022)</a>
	      <br>
              <br>
              Staff Member, <a href="https://sites.google.com/view/telluride2018/home">Neuromorphic Cognition Engineering Workshop (July 2018)</a>	        
            </td>
          </tr>
	  </tbody></table>


<!-- Footer -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                 Thanks to <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a> for this minimalist website template.
                </font>
              </p>
            </td>
          </tr>
        </table>        
        </td>
      </tr>
    </table>
    <script xml:space="preserve" language="JavaScript">
      hideallbibs();
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('3dpaper_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('rwfm_abs');
    </script>
    <script xml:space="preserve" language="JavaScript">
      hideblock('ads_abs');
    </script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-102204172-1', 'auto');
  ga('send', 'pageview');

</script>
  </body>
</html>
